{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline adversarial attacks for LMS CNN and ESC-10\n",
    "Performing 4 typical adversarial attacks.\n",
    "\n",
    "Target model: [Ahmed et al 2020](https://www.researchgate.net/publication/344519283_Automatic_Environmental_Sound_Recognition_AESR_Using_Convolutional_Neural_Network)\n",
    "\n",
    "Data: [ESC-10](https://github.com/karolpiczak/ESC-50)\n",
    "\n",
    "Attacks: [FGSM](https://arxiv.org/abs/1412.6572), [BIM](https://arxiv.org/abs/1607.02533), [Deepfool](https://arxiv.org/abs/1511.04599), [Carlini & Wagner](https://arxiv.org/abs/1608.04644)\n",
    "\n",
    "Attack implementations based on [torchattacks](https://github.com/Harry24k/adversarial-attacks-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"Data\"\n",
    "DATASET = \"ESC-10\"\n",
    "\n",
    "DATASET_PATH = os.path.join(DATA_DIR, DATASET)\n",
    "META_PATH = os.path.join(DATASET_PATH, \"meta\", \"esc10.csv\")\n",
    "AUDIO_PATH = os.path.join(DATASET_PATH, \"audio\")\n",
    "\n",
    "WEIGHTS_DIR = \"Weights\"\n",
    "\n",
    "SAMPLES_DIR = \"Samples\"\n",
    "\n",
    "MODEL = \"cnn_lms\"\n",
    "\n",
    "CHECKPT_PATH = os.path.join(WEIGHTS_DIR, \"cnn_best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(CHECKPT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target model\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=\"valid\"),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=\"same\"),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.l3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=\"same\"),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.l4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=\"same\"),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.l5 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=128 * 7 * 7, out_features=512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.l6 = nn.Sequential(\n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.Linear(in_features=512, out_features=10)\n",
    "        )\n",
    "    \n",
    "        self.sf = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = self.l1(input_data)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        x = self.l5(x)\n",
    "\n",
    "        logits = self.l6(x)\n",
    "        probs = self.sf(logits)\n",
    "\n",
    "        return logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 126, 126]             160\n",
      "         MaxPool2d-2           [-1, 16, 63, 63]               0\n",
      "           Dropout-3           [-1, 16, 63, 63]               0\n",
      "              ReLU-4           [-1, 16, 63, 63]               0\n",
      "            Conv2d-5           [-1, 32, 63, 63]           4,640\n",
      "         MaxPool2d-6           [-1, 32, 31, 31]               0\n",
      "           Dropout-7           [-1, 32, 31, 31]               0\n",
      "              ReLU-8           [-1, 32, 31, 31]               0\n",
      "            Conv2d-9           [-1, 64, 31, 31]          18,496\n",
      "        MaxPool2d-10           [-1, 64, 15, 15]               0\n",
      "          Dropout-11           [-1, 64, 15, 15]               0\n",
      "             ReLU-12           [-1, 64, 15, 15]               0\n",
      "           Conv2d-13          [-1, 128, 15, 15]          73,856\n",
      "        MaxPool2d-14            [-1, 128, 7, 7]               0\n",
      "          Dropout-15            [-1, 128, 7, 7]               0\n",
      "             ReLU-16            [-1, 128, 7, 7]               0\n",
      "          Flatten-17                 [-1, 6272]               0\n",
      "          Dropout-18                 [-1, 6272]               0\n",
      "           Linear-19                  [-1, 512]       3,211,776\n",
      "             ReLU-20                  [-1, 512]               0\n",
      "          Dropout-21                  [-1, 512]               0\n",
      "           Linear-22                   [-1, 10]           5,130\n",
      "          Softmax-23                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 3,314,058\n",
      "Trainable params: 3,314,058\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 6.33\n",
      "Params size (MB): 12.64\n",
      "Estimated Total Size (MB): 19.04\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN().to(device)\n",
    "summary(cnn, (1, 128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attacks\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FGSM(model, samples, labels, eps, loss_fn):\n",
    "    samples = samples.clone().detach().requires_grad_(True)\n",
    "    labels = labels.clone().detach()\n",
    "\n",
    "    outputs, _ = model(samples)\n",
    "\n",
    "    # Calculate loss\n",
    "    cost = loss_fn(outputs, labels)\n",
    "\n",
    "    # Update adversarial images\n",
    "    grad = torch.autograd.grad(\n",
    "        cost, samples, retain_graph=False, create_graph=False\n",
    "    )[0]\n",
    "\n",
    "    adv_samples = samples + eps*grad.sign()\n",
    "    adv_samples = torch.clamp(adv_samples, min=0, max=1).detach()\n",
    "\n",
    "    return adv_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIM(model, samples, labels, alpha, eps, steps, loss_fn):\n",
    "    original_samples = samples.clone().detach()\n",
    "    samples = samples.clone().detach()\n",
    "    labels = labels.clone().detach()\n",
    "\n",
    "    samples_min = original_samples - eps\n",
    "    samples_max = original_samples + eps\n",
    "\n",
    "    for _ in range(steps):\n",
    "        samples.requires_grad_(True)\n",
    "        outputs, _ = model(samples)\n",
    "\n",
    "        cost = loss_fn(outputs, labels)\n",
    "        grad = torch.autograd.grad(\n",
    "            cost, samples, retain_graph=False, create_graph=False\n",
    "        )[0]\n",
    "\n",
    "        adv_samples = samples + alpha * grad.sign()\n",
    "        adv_samples = torch.clamp(adv_samples, min=samples_min, max=samples_max)\n",
    "        samples = torch.clamp(adv_samples, min=0, max=1).detach()\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepFool(model, samples, labels, steps, overshoot):\n",
    "    samples = samples.clone().detach()\n",
    "    labels = labels.clone().detach()\n",
    "\n",
    "    batch_size = len(samples)\n",
    "    correct = torch.tensor([True]*batch_size)\n",
    "    target_labels = labels.clone().detach()\n",
    "    curr_steps = 0\n",
    "\n",
    "    adv_samples = []\n",
    "    for idx in range(batch_size):\n",
    "        sample = samples[idx:idx+1].clone().detach()\n",
    "        adv_samples.append(sample)\n",
    "\n",
    "    while (True in correct) and (curr_steps < steps):\n",
    "        for idx in range(batch_size):\n",
    "            if not correct[idx]: continue\n",
    "            early_stop, pre, adv_sample = df_indiv(model, adv_samples[idx], labels[idx], overshoot)\n",
    "            adv_samples[idx] = adv_sample\n",
    "            target_labels[idx] = pre\n",
    "            if early_stop:\n",
    "                correct[idx] = False\n",
    "        curr_steps += 1\n",
    "\n",
    "    adv_samples = torch.cat(adv_samples).detach()\n",
    "    return adv_samples\n",
    "\n",
    "\n",
    "def df_indiv(model, sample, label, overshoot):\n",
    "    sample.requires_grad_(True)\n",
    "    fs = model(sample)[0][0]\n",
    "    prob = torch.argmax(fs, dim=0)\n",
    "    if prob != label:\n",
    "        return (True, prob, sample)\n",
    "\n",
    "    ws = construct_jacobian(fs, sample)\n",
    "    sample = sample.detach()\n",
    "\n",
    "    f_0 = fs[label]\n",
    "    w_0 = ws[label]\n",
    "\n",
    "    wrong_classes = [i for i in range(len(fs)) if i != label]\n",
    "    f_k = fs[wrong_classes]\n",
    "    w_k = ws[wrong_classes]\n",
    "\n",
    "    f_prime = f_k - f_0\n",
    "    w_prime = w_k - w_0\n",
    "    value = torch.abs(f_prime) \\\n",
    "            / torch.norm(nn.Flatten()(w_prime), p=2, dim=1)\n",
    "    hat_L = torch.argmin(value, 0)\n",
    "\n",
    "    delta = (torch.abs(f_prime[hat_L]) * w_prime[hat_L] \\\n",
    "            / (torch.norm(w_prime[hat_L], p=2) ** 2))\n",
    "\n",
    "    target_label = hat_L if hat_L < label else hat_L + 1\n",
    "\n",
    "    adv_sample = sample + (1+overshoot)*delta\n",
    "    adv_sample = torch.clamp(adv_sample, min=0, max=1).detach()\n",
    "    return (False, target_label, adv_sample)\n",
    "\n",
    "# https://stackoverflow.com/questions/63096122/pytorch-is-it-possible-to-differentiate-a-matrix\n",
    "# torch.autograd.functional.jacobian is only for torch >= 1.5.1\n",
    "def construct_jacobian(y, x):\n",
    "    x_grads = []\n",
    "    for idx, y_element in enumerate(y):\n",
    "        if x.grad is not None:\n",
    "            x.grad.zero_()\n",
    "        y_element.backward(retain_graph=(False or idx+1 < len(y)))\n",
    "        x_grads.append(x.grad.clone().detach())\n",
    "    return torch.stack(x_grads).reshape(*y.shape, *x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CW(model, samples, labels, c, kappa, steps, learning_rate):\n",
    "    samples = samples.clone().detach()\n",
    "    labels = labels.clone().detach()\n",
    "\n",
    "    # w = torch.zeros_like(images).detach() # Requires 2x times\n",
    "    w = inverse_tanh_space(samples).detach()\n",
    "    w.requires_grad_(True)\n",
    "\n",
    "    best_adv_samples = samples.clone().detach()\n",
    "    best_L2 = 1e10 * torch.ones((len(samples))).to(samples.device)\n",
    "    prev_cost = 1e10\n",
    "    dim = len(samples.shape)\n",
    "\n",
    "    MSELoss = nn.MSELoss(reduction='none')\n",
    "    Flatten = nn.Flatten()\n",
    "\n",
    "    optimizer = optim.Adam([w], lr=learning_rate)\n",
    "\n",
    "    for step in range(steps):\n",
    "        # Get adversarial images\n",
    "        adv_samples = tanh_space(w)\n",
    "\n",
    "        # Calculate loss\n",
    "        current_L2 = MSELoss(Flatten(adv_samples),\n",
    "                             Flatten(samples)).sum(dim=1)\n",
    "        L2_loss = current_L2.sum()\n",
    "\n",
    "        outputs, _ = model(adv_samples)\n",
    "        f_loss = f(outputs, labels, kappa).sum()\n",
    "\n",
    "        cost = L2_loss + c*f_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update adversarial images\n",
    "        pre = torch.argmax(outputs.detach(), 1)\n",
    "        condition = (pre != labels).float()\n",
    "\n",
    "        # Filter out images that get either correct predictions or non-decreasing loss,\n",
    "        # i.e., only images that are both misclassified and loss-decreasing are left\n",
    "        mask = condition * (best_L2 > current_L2.detach())\n",
    "        best_L2 = mask * current_L2.detach() + (1 - mask) * best_L2\n",
    "\n",
    "        mask = mask.view([-1] + [1] * (dim - 1))\n",
    "        best_adv_samples = mask * adv_samples.detach() + (1 - mask) * best_adv_samples\n",
    "\n",
    "        # Early stop when loss does not converge.\n",
    "        # max(.,1) To prevent MODULO BY ZERO error in the next step.\n",
    "        if step % max(steps//10, 1) == 0:\n",
    "            if cost.item() > prev_cost:\n",
    "                return best_adv_samples\n",
    "            prev_cost = cost.item()\n",
    "\n",
    "    return best_adv_samples\n",
    "\n",
    "def tanh_space(x):\n",
    "    return (1 / 2) * (torch.tanh(x) + 1)\n",
    "\n",
    "def inverse_tanh_space(x):\n",
    "    # torch.atanh is only for torch >= 1.7.0\n",
    "    # atanh is defined in the range -1 to 1\n",
    "    return atanh(torch.clamp(x * 2 - 1, min=-1, max=1))\n",
    "\n",
    "def atanh(x):\n",
    "    return 0.5 * torch.log((1 + x) / (1 - x))\n",
    "\n",
    "# f-function in the paper\n",
    "def f(outputs, labels, kappa):\n",
    "    one_hot_labels = torch.eye(outputs.shape[1]).to(outputs.device)[labels]\n",
    "\n",
    "    # find the max logit other than the target class\n",
    "    other = torch.max((1-one_hot_labels)*outputs, dim=1)[0]\n",
    "    # get the target class's logit\n",
    "    real = torch.max(one_hot_labels*outputs, dim=1)[0]\n",
    "\n",
    "    return torch.clamp((real-other), min=-kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean examples\n",
    "___\n",
    "The attacks will be performed on correctly classified test samples from ESC-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_to_label = {0: \"dog\", 41: \"chainsaw\", 12: \"crackling_fire\", 40: \"helicopter\", 10: \"rain\",\n",
    "                   20: \"crying_baby\", 38: \"clock_tick\", 21: \"sneezing\", 1: \"rooster\", 11: \"sea_waves\"}\n",
    "label_to_target = {\"dog\": 0, \"chainsaw\": 41, \"crackling_fire\": 12, \"helicopter\": 40, \"rain\": 10,\n",
    "                   \"crying_baby\": 20, \"clock_tick\": 38, \"sneezing\": 21, \"rooster\": 1, \"sea_waves\": 11}\n",
    "target_to_y = {0: 0, 41:1, 12: 2, 40: 3, 10: 4, 20: 5, 38: 6, 21: 7, 1: 8, 11: 9}\n",
    "y_to_target = {0: 0, 1:41, 2: 12, 3: 40, 4: 10, 5: 20, 6: 38, 7: 21, 8: 1, 9: 11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESC10(Dataset):\n",
    "    def __init__(self, meta, transformation=None):\n",
    "        self.meta = meta\n",
    "        self.transformation = transformation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        spec = torch.from_numpy(self.meta.loc[index, \"spectrogram\"]).unsqueeze(0)\n",
    "        if self.transformation is not None:\n",
    "            spec = self.transformation(spec)\n",
    "        target = self.meta.loc[index, \"target\"]\n",
    "\n",
    "        return spec, target_to_y[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5-151085-A-20.wav</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5-170338-A-41.wav</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5-170338-B-41.wav</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5-171653-A-41.wav</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5-177957-A-40.wav</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>5-233160-A-1.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>5-234879-A-1.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>5-234879-B-1.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>5-235671-A-38.wav</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>5-9032-A-0.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             filename  target\n",
       "0   5-151085-A-20.wav      20\n",
       "1   5-170338-A-41.wav      41\n",
       "2   5-170338-B-41.wav      41\n",
       "3   5-171653-A-41.wav      41\n",
       "4   5-177957-A-40.wav      40\n",
       "..                ...     ...\n",
       "75   5-233160-A-1.wav       1\n",
       "76   5-234879-A-1.wav       1\n",
       "77   5-234879-B-1.wav       1\n",
       "78  5-235671-A-38.wav      38\n",
       "79     5-9032-A-0.wav       0\n",
       "\n",
       "[80 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(META_PATH)\n",
    "clean_samples_meta = dataset[dataset[\"fold\"] == 5].reset_index(drop=True)\n",
    "clean_samples_meta = clean_samples_meta.drop(columns=[\"fold\", \"category\", \"src_file\", \"take\"])\n",
    "clean_samples_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "      <th>spectrogram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5-151085-A-20.wav</td>\n",
       "      <td>20</td>\n",
       "      <td>[[-42.345238, -42.922577, -53.057945, -65.3477...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5-170338-A-41.wav</td>\n",
       "      <td>41</td>\n",
       "      <td>[[-31.258703, -35.422966, -37.695583, -37.3043...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5-170338-B-41.wav</td>\n",
       "      <td>41</td>\n",
       "      <td>[[-39.22971, -48.085846, -52.76058, -57.827343...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5-171653-A-41.wav</td>\n",
       "      <td>41</td>\n",
       "      <td>[[-51.152584, -49.817825, -51.062927, -51.8161...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5-177957-A-40.wav</td>\n",
       "      <td>40</td>\n",
       "      <td>[[-22.853165, -8.151789, -0.7906742, -5.289486...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  target  \\\n",
       "0  5-151085-A-20.wav      20   \n",
       "1  5-170338-A-41.wav      41   \n",
       "2  5-170338-B-41.wav      41   \n",
       "3  5-171653-A-41.wav      41   \n",
       "4  5-177957-A-40.wav      40   \n",
       "\n",
       "                                         spectrogram  \n",
       "0  [[-42.345238, -42.922577, -53.057945, -65.3477...  \n",
       "1  [[-31.258703, -35.422966, -37.695583, -37.3043...  \n",
       "2  [[-39.22971, -48.085846, -52.76058, -57.827343...  \n",
       "3  [[-51.152584, -49.817825, -51.062927, -51.8161...  \n",
       "4  [[-22.853165, -8.151789, -0.7906742, -5.289486...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectrograms = []\n",
    "\n",
    "for filename in clean_samples_meta[\"filename\"]:\n",
    "    wavpath = os.path.join(AUDIO_PATH, filename)\n",
    "    waveform, _ = librosa.load(wavpath, sr=SR)\n",
    "    mel = librosa.feature.melspectrogram(y=waveform, n_fft=1024, win_length=800, hop_length=400, sr=SR)\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "    mel_db = librosa.util.fix_length(mel_db, axis=1, size=128)\n",
    "    spectrograms.append(np.clip(mel_db, a_min=None, a_max=0.))\n",
    "\n",
    "clean_samples_meta[\"spectrogram\"] = spectrograms\n",
    "clean_samples_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_minmax = lambda x: (x + 80.) / 80.\n",
    "tf_inv_minmax = lambda x: x * 80. - 80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_samples_data = ESC10(clean_samples_meta, transformation=tf_minmax)\n",
    "clean_samples_loader = DataLoader(clean_samples_data, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack evaluation\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack parameters\n",
    "params = {\n",
    "    \"eps\": 0.05,\n",
    "    \"alpha\": 0.005,\n",
    "    \"steps\": 12,\n",
    "    \"max_iters\": 1000,\n",
    "    \"overshoot\": 0.02,\n",
    "    \"c\": 1.,\n",
    "    \"kappa\": 0,\n",
    "    \"learning_rate\": 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def craft_adv_samples(model, samples, labels, loss_fn, params, attack):\n",
    "    if attack == \"FGSM\":\n",
    "        pert_data = FGSM(model, samples, labels, params[\"eps\"], loss_fn)\n",
    "        \n",
    "    if attack == \"BIM\":\n",
    "        pert_data = BIM(model, samples, labels, params[\"alpha\"],\n",
    "                        params[\"eps\"], params[\"steps\"], loss_fn)\n",
    "        \n",
    "    if attack == \"DeepFool\":\n",
    "        pert_data = DeepFool(model, samples, labels, params[\"steps\"], params[\"overshoot\"])\n",
    "        \n",
    "    if attack == \"CW\":\n",
    "        pert_data = CW(model, samples, labels, params[\"c\"],\n",
    "                       params[\"kappa\"], params[\"max_iters\"],\n",
    "                       params[\"learning_rate\"])\n",
    "        \n",
    "    return pert_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_model_performance(model, test_loader, loss_fn, device, params, attack=None):\n",
    "    model.eval()\n",
    "    num_correct_pred = 0\n",
    "    \n",
    "    for X, y_true in tqdm(test_loader):\n",
    "        X = X.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, y_probs = model(X)\n",
    "        y_labels = torch.argmax(y_probs, 1)\n",
    "        \n",
    "        if attack != None:\n",
    "            pert_data = craft_adv_samples(model, X, y_true, loss_fn, params, attack)\n",
    "            with torch.no_grad():\n",
    "                _, y_probs = model(pert_data)\n",
    "            y_labels = torch.argmax(y_probs, 1)\n",
    "        num_correct_pred += (y_labels == y_true).sum()\n",
    "\n",
    "    accuracy = num_correct_pred / len(test_loader.dataset)\n",
    "    return model, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c514665c0c5746509a74805aa4a21497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on clean examples: 85.00%\n"
     ]
    }
   ],
   "source": [
    "model, accuracy = measure_model_performance(model, clean_samples_loader, loss_fn, device, params)\n",
    "print(f\"Accuracy on clean examples: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3577cf267954e4e8e22695eb0857ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on FGSM examples: 25.00%\n"
     ]
    }
   ],
   "source": [
    "model, accuracy = measure_model_performance(model, clean_samples_loader, loss_fn, device, params, attack='FGSM')\n",
    "print(f\"Accuracy on FGSM examples: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0453c623214457ad2398234618a80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on BIM examples: 3.75%\n"
     ]
    }
   ],
   "source": [
    "model, accuracy = measure_model_performance(model, clean_samples_loader, loss_fn, device, params, attack='BIM')\n",
    "print(f\"Accuracy on BIM examples: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5d4476489a430bad0642da25c5172c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on DeepFool examples: 12.50%\n"
     ]
    }
   ],
   "source": [
    "model, accuracy = measure_model_performance(model, clean_samples_loader, loss_fn, device, params, attack='DeepFool')\n",
    "print(f\"Accuracy on DeepFool examples: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55d677c34fc467b87dc1a08811a063d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on CW examples: 0.00%\n"
     ]
    }
   ],
   "source": [
    "model, accuracy = measure_model_performance(model, clean_samples_loader, loss_fn, device, params, attack='CW')\n",
    "print(f\"Accuracy on CW examples: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Attack = [\"FGSM\", \"BIM\", \"DeepFool\", \"CW\"]\n",
    "\n",
    "correct = []\n",
    "\n",
    "Samples = {key:torch.empty((0, 1, 128, 128)) for key in Attack}\n",
    "Samples[\"Clean\"] = torch.empty((0, 1, 128, 128))\n",
    "\n",
    "model.eval()\n",
    "for X, y_true in clean_samples_loader:\n",
    "    X = X.to(device)\n",
    "    y_true = y_true.to(device)\n",
    "    with torch.no_grad():\n",
    "        _, y_probs = model(X)\n",
    "    y_labels = torch.argmax(y_probs, 1)\n",
    "\n",
    "    correct.extend(list((y_labels == y_true).bool().cpu().numpy()))\n",
    "\n",
    "    X = X[y_labels == y_true]\n",
    "    y_labels = y_labels[y_labels == y_true]\n",
    "\n",
    "    Samples[\"Clean\"] = torch.cat((Samples[\"Clean\"], X.cpu()))\n",
    "\n",
    "    for atk in Attack:\n",
    "        pert_data = craft_adv_samples(model, X, y_labels, loss_fn, params, atk)\n",
    "        Samples[atk] = torch.cat((Samples[atk], pert_data.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5-151085-A-20.wav</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5-170338-A-41.wav</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5-170338-B-41.wav</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5-171653-A-41.wav</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5-177957-A-40.wav</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  target\n",
       "0  5-151085-A-20.wav      20\n",
       "1  5-170338-A-41.wav      41\n",
       "2  5-170338-B-41.wav      41\n",
       "3  5-171653-A-41.wav      41\n",
       "4  5-177957-A-40.wav      40"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_samples_meta[\"correct_prediction\"] = correct\n",
    "samples_meta = clean_samples_meta[clean_samples_meta[\"correct_prediction\"] == True].reset_index(drop=True)\n",
    "samples_meta = samples_meta.drop(columns=[\"correct_prediction\", \"spectrogram\"])\n",
    "\n",
    "samples_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_path = os.path.join(SAMPLES_DIR, MODEL)\n",
    "\n",
    "if not os.path.exists(samples_path):\n",
    "    os.makedirs(samples_path)\n",
    "\n",
    "for sample_type in Samples.keys():\n",
    "    torch.save(Samples[sample_type], os.path.join(samples_path, \"_\".join([sample_type, \"samples.pt\"])))\n",
    "\n",
    "samples_meta.to_csv(os.path.join(samples_path, \"samples_meta.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
